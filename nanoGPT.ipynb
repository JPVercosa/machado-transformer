{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Machado's text: 10977785 characters\n"
     ]
    }
   ],
   "source": [
    "machado_path = 'kaggle/input/machado-de-assis/machado.txt'\n",
    "with open (machado_path, 'r', encoding='utf-8') as f:\n",
    "    machado = f.read()\n",
    "\n",
    "print(\"Length of Machado's text: {} characters\".format(len(machado)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with 10 million characters that in some way encode the manner that machado writes it texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"$%&'()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz§ª°ÀÁÂÃÇÈÉÊËÍÓÔÕÚÛÜàáâãäçèéêëìíîïñòóôõöùúûüœ\n",
      "There are 127 unique characters in the Machado corpus\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(machado)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('There are {} unique characters in the Machado corpus'.format(vocab_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building basic Encoder and Decoder\n",
    "Encoder get a string and map each character to an integer. <br>\n",
    "Decoder get a list of integers and map each integer to a character\n",
    "\n",
    "This is the way that we choose to tokenize our pieces of word. <br>\n",
    "Google uses \"SentencePiece\" and OpenAi uses \"tiktoken\". <br>\n",
    "Those encoders have a different vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 57, 59, 64, 57, 60, 71, 1, 60, 61, 1, 30, 75, 75, 65, 75]\n",
      "Machado de Assis\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)} # dictionary mapping characters to integers\n",
    "itos = {i:ch for i,ch in enumerate(chars)} # dictionary mapping integers to characters\n",
    "encode = lambda s: [stoi[ch] for ch in s] # encode string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode list of integers to string\n",
    "\n",
    "print(encode(\"Machado de Assis\"))\n",
    "print(decode(encode(\"Machado de Assis\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10977785]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "machado_data = torch.tensor(encode(machado), dtype=torch.long)\n",
    "print(machado_data.shape, machado_data.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spliting data\n",
    "Splitting data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(machado_data) * 0.7)\n",
    "train_data = machado_data[:n]\n",
    "val_data = machado_data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Batches and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "\n",
    "def get_batch(split):\n",
    "    # split can be 'train' or 'val'\n",
    "    # generate small random batch of data of input x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])     # create an x matrix of size (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # create an y matrix of size (batch_size, block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Loss\n",
    "\n",
    "We do this because as we are working with batches, we can get more lucky in some bathes than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the model structure\n",
    "\n",
    "Classes: <br>\n",
    "- Head\n",
    "- MultiHeadAttention\n",
    "- FeedForward\n",
    "- Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    ''' one head of self-attention mechanism'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x has shape (batch_size, time-step, channels)\n",
    "        # output has shape (batch_size, time-step, head_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # compute attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform attention of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' multi-head attention mechanism'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    ''' simple linear layer followed by a non-linear activation function'''\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    ''' transformer block'''\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd) \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size) # final linear layer\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T) -> (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T) -> (T, n_embd)\n",
    "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
    "        x = self.blocks(x) # (B, T, n_embd)\n",
    "        x = self.ln_f(x) # (B, T, n_embd)\n",
    "        logits = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            #print(logits.shape, targets.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            #print(logits.shape, targets.shape)\n",
    "            loss = F.cross_entropy(logits, targets) #, ignore_index=0\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ''' generate new tokens from the given tokens'''\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel / number of sequences in a batch (we can process batches in parallel) \n",
    "block_size = 256 # what is the maximum context length for predictions? / length of a sequence (maximum context lengt)\n",
    "max_iters = 5000\n",
    "eval_interval = 250\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "#Check device (CPU or 'cuda' (GPU)) (GPU is recommended)\n",
    "print('device:', device)\n",
    "#Check if CUDA is correct installed\n",
    "#print(torch.zeros(1).cuda())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10836607 parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.9791, val loss 4.9801\n",
      "step 250: train loss 2.3953, val loss 2.4041\n",
      "step 500: train loss 2.0462, val loss 2.0667\n",
      "step 750: train loss 1.7813, val loss 1.8254\n",
      "step 1000: train loss 1.6110, val loss 1.6733\n",
      "step 1250: train loss 1.5155, val loss 1.5820\n",
      "step 1500: train loss 1.4593, val loss 1.5314\n",
      "step 1750: train loss 1.4158, val loss 1.4977\n",
      "step 2000: train loss 1.3847, val loss 1.4693\n",
      "step 2250: train loss 1.3623, val loss 1.4521\n",
      "step 2500: train loss 1.3370, val loss 1.4298\n",
      "step 2750: train loss 1.3164, val loss 1.4139\n",
      "step 3000: train loss 1.3066, val loss 1.4101\n",
      "step 3250: train loss 1.2873, val loss 1.3938\n",
      "step 3500: train loss 1.2768, val loss 1.3775\n",
      "step 3750: train loss 1.2654, val loss 1.3711\n",
      "step 4000: train loss 1.2625, val loss 1.3752\n",
      "step 4250: train loss 1.2481, val loss 1.3616\n",
      "step 4500: train loss 1.2423, val loss 1.3507\n",
      "step 4750: train loss 1.2349, val loss 1.3489\n",
      "step 4999: train loss 1.2281, val loss 1.3387\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate text from the model\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # start with a single <BOS> token      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "que a injustiça perdeu o assunto.\n",
      "- É então. Assim, em o ilto daquela emitidão atendeu a\n",
      "comissão das suas buisas, em meu anos, para que meterei nós ao\n",
      "Político, no contragou um em quando o a o levou a carta e obrar; e vinha\n",
      "este conto dentro do último drama, se não asendava repõia na véspera, e há um leite,\n",
      "é um deputado ficar prancipiamente para viver um armandontino. Flora o depututado a\n",
      "Capital é Pianoi e Foi encontreiramente do paço que voltava sementira, porque se ele uma\n",
      "sistema das esperanças, a de mim. Ao que deixasse bem esse tempo abrindado, e o que\n",
      "entraria no desejo da última produzia: afastas da dor; visita\n",
      "depois, acabar que as páginas. E trazia alguns cens enfiores de alvos os trocos,\n",
      "todavia consentir andar uma noite de poder; viu-a falecer a consciência. Pedir-me-ia\n",
      "aos outros dos seus louderais, e dem que não só do acontece.\n",
      "Ago-lhe que é que até vivia à manhã de Cruziu.\n",
      "A bordo perpétua fazia um capítulo para dizer\n",
      "um pouco, nela a vizinhança; era já igualmente alegres, cada parte do\n",
      "triste para ele; e, dilamando aqoelo, pouco confiara abalar, boa senhora dela, tambar com\n",
      "os introsques e iam à direção das comuns. A h, amigo, julgava-me, muitas vezes as\n",
      "mãos, e o fogo do papas levaria-os para ler o osstros artigos... Minh'alma\n",
      "mocida às vezes! Não!\n",
      "FIX\n",
      "CAPÍTULO XL CII\n",
      "TITO PIGUIDOS\n",
      "Nábs? (ao do passo, como procurar aqui penúadista.\n",
      "MARRIO DO BOSTOS DO NETESMU\n",
      "J. EL\n",
      "Quem quero saber... Há maças que eu não desse músico é grave:\n",
      "- Pois sabem que eu respondeu elogo? pergunta que eu, e então com isso, simpatizeses apreciança alegura,\n",
      "quase não esta idade, me até posso aos orar o chamar às vezes uma que lá\n",
      "pende um sosseio, que a primeira poiciplicamente lhe encontrava em menos, e como se consentiri\n",
      "em si lrapizar; era quase o que dizia que precioava, naquele cleremo se o jogo. Por abricular ordenal a minha\n",
      "face, que eu padecem enhuma. Chamo-lhe Gil pi-me que lhe conta só por não poucas esposas,\n",
      "amante alguma perversuam, e haja de que sejam mudanças é aparência que tenho roni-te prata,\n",
      "cruspo; e é bom compor-vou-o.... e fez aqui uma experiente.\n",
      "Talvez dera a prova resistituir...\n",
      "O fim de dos seu mundo. Nunca ces tu, mas você tenho homido em que força; é\n",
      "doutor um desmasio maior. Sinto comem de um ato.\n",
      "- Isto será incanso de propor, não eu desejo.\n",
      "Ai bebaí de salvar págio, o médico da pálido sem nossaeio estranha.\n",
      "Reforma-se preocupado do tropôr daquela serra, estando e ficou só que ele vivia\n",
      "abaitado, recebeste, em janela à mesa, que a publica\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(context, max_new_tokens=2500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/all_machado.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe3334bf2b7b12fca4183da11a556de6e9d4078d5506996905d19c2663a69af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
