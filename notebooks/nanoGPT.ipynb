{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Machado's text: 10977986 characters\n"
     ]
    }
   ],
   "source": [
    "machado_path = '../data/machado.txt'\n",
    "with open (machado_path, 'r', encoding='utf-8') as f:\n",
    "    machado = f.read()\n",
    "\n",
    "print(\"Length of Machado's text: {} characters\".format(len(machado)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with 10 million characters that in some way encode the manner that machado writes it texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"$%&'()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyzª°Çàáâãçéêíóôõú\n",
      "There are 98 unique characters in the Machado corpus\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(machado)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('There are {} unique characters in the Machado corpus'.format(vocab_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building basic Encoder and Decoder\n",
    "Encoder get a string and map each character to an integer. <br>\n",
    "Decoder get a list of integers and map each integer to a character\n",
    "\n",
    "This is the way that we choose to tokenize our pieces of word. <br>\n",
    "Google uses \"SentencePiece\" and OpenAi uses \"tiktoken\". <br>\n",
    "Those encoders have a different vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 57, 59, 64, 57, 60, 71, 1, 60, 61, 1, 30, 75, 75, 65, 75]\n",
      "Machado de Assis\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)} # dictionary mapping characters to integers\n",
    "itos = {i:ch for i,ch in enumerate(chars)} # dictionary mapping integers to characters\n",
    "encode = lambda s: [stoi[ch] for ch in s] # encode string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode list of integers to string\n",
    "\n",
    "print(encode(\"Machado de Assis\"))\n",
    "print(decode(encode(\"Machado de Assis\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10977986]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "machado_data = torch.tensor(encode(machado), dtype=torch.long)\n",
    "print(machado_data.shape, machado_data.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spliting data\n",
    "Splitting data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(machado_data) * 0.7)\n",
    "train_data = machado_data[:n]\n",
    "val_data = machado_data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import Config\n",
    "\n",
    "batch_size = Config.batch_size\n",
    "block_size = Config.block_size\n",
    "max_iters = Config.max_iters\n",
    "eval_interval = Config.eval_interval\n",
    "learning_rate = Config.learning_rate\n",
    "device = Config.device\n",
    "eval_iters = Config.eval_iters\n",
    "n_embd = Config.n_embd\n",
    "n_head = Config.n_head\n",
    "n_layer = Config.n_layer\n",
    "dropout = Config.dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Batches and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "\n",
    "def get_batch(split):\n",
    "    # split can be 'train' or 'val'\n",
    "    # generate small random batch of data of input x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])     # create an x matrix of size (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # create an y matrix of size (batch_size, block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Loss\n",
    "\n",
    "We do this because as we are working with batches, we can get more lucky in some bathes than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the model structure\n",
    "\n",
    "Classes: <br>\n",
    "- Head\n",
    "- MultiHeadAttention\n",
    "- FeedForward\n",
    "- Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    ''' one head of self-attention mechanism'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x has shape (batch_size, time-step, channels)\n",
    "        # output has shape (batch_size, time-step, head_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # compute attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform attention of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' multi-head attention mechanism'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    ''' simple linear layer followed by a non-linear activation function'''\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    ''' transformer block'''\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd) \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size) # final linear layer\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T) -> (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T) -> (T, n_embd)\n",
    "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
    "        x = self.blocks(x) # (B, T, n_embd)\n",
    "        x = self.ln_f(x) # (B, T, n_embd)\n",
    "        logits = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            #print(logits.shape, targets.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            #print(logits.shape, targets.shape)\n",
    "            loss = F.cross_entropy(logits, targets) #, ignore_index=0\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ''' generate new tokens from the given tokens'''\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10814306 parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5767, val loss 4.5790\n",
      "step 500: train loss 2.0149, val loss 2.0372\n",
      "step 1000: train loss 1.5859, val loss 1.6467\n",
      "step 1500: train loss 1.4498, val loss 1.5296\n",
      "step 2000: train loss 1.3761, val loss 1.4622\n",
      "step 2500: train loss 1.3303, val loss 1.4224\n",
      "step 2999: train loss 1.3047, val loss 1.4022\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate text from the model\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # start with a single <BOS> token      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "do para compr do amor. José Dias,\n",
      "aprovável\n",
      "que ele entrara a risalogia ou dele coração do jardim. Com ela as penas venturas.\n",
      "Da arma briscova era endírita,\n",
      "mas falava separar\n",
      "nem a medrosa de hino, que era segu trêmulo.\n",
      "Esta france que tipejasse as porveidades; fui, e a manhã história da\n",
      "mínima e o podem distante ao papel, Suculo\n",
      "wenllar e persa organdesse celebrar anos de teus altos que fossem prendem?\n",
      "Teu marido eultor, humana, meio melhor do companhades à tintudo de perder que reconhecia.\n",
      "Evaigo ainda sorriu, reendo a voz, dadinhando\n",
      "os homes, uma dificuldade subilidade; a tardução desse concerta a caso de Viscola, - continuou:\n",
      "amoelhando, umas espécies do Castal volveiro, mais fonte quando\n",
      "escreveu iamigos no velho discurso. Ouviu-se vir vejando o esprau. Ah!\n",
      "Camalhão salve, Rubião amava o perdéado, escondeu uma cor e,\n",
      "fogestos de cruzer, que pareceu tsmuefaridamente.\n",
      "- Acho quência tão! disse Era uma palavra contra os teatros S, porque,\n",
      "malém todo se conservi à ciênica de Helena; mas a tive que não faltaria um ralhe culpação, o\n",
      "interculuxidiatamente de medida com a modéstia de uma razão de fogoía um\n",
      "tempo, e\n",
      "impressão essa vidrata referidação em mança - o outro\n",
      "de corto, em todas bociações transbo da riscondemos, altas; superdar ter\n",
      "altado pela de Lágara, não me memóe, como rémal, não hi aprovém, mas sem\n",
      "pmblhente.\n",
      "Posso abdiu no segundo amigo quanto vissem na meignita\n",
      "cortalmente. O arrespivamente trepara que alguém os ódios viviam e\n",
      "encomendava na sala da fedunção da classidade.\n",
      "Mas não estariam homem de dito concordo daquela quiedade noiva; respirou muito,\n",
      "ao prábio azevedo nos olhos na essa sua falação bem\n",
      "seleito.\n",
      "FIqui ela no passar, Estácio do Projo, eis em dia com sala, não\n",
      "ano não fogo de gente (Escopa) de tristezar encapatado. Há,\n",
      "conveniênuemos que serviaria uma estrúpula da santa.\n",
      "A feição do laço farejo, mas obiguemos logo a atenção do tempo, a outra tarde,\n",
      "e seria uns escolhaçam com a amoção e o geraal não teve dizer o iguaí em que a\n",
      "opreciosa a moça ali mudada pela perguntas qualidade da\n",
      "circulação outra, não foi que é certo da minha\n",
      "língua e a potei-o\n",
      "farto. O quem me izia para o seu primeiro mérito do padece da rede. Ao menos, refortal à\n",
      "noite e os segundos; mas aeio se mentirou. O que é a dois de tua\n",
      "voltas, e acudiu descobertamente também era a confieção recuso e fialar pressa, para\n",
      "a grasla, e devendendo as luas, ou passar uma carta, por que perde, ou incolém\n",
      "nas noites sucedes, a bastado da nintempéria do outro, como se\n",
      "barrecer \n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(context, max_new_tokens=2500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/nb_machado.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe3334bf2b7b12fca4183da11a556de6e9d4078d5506996905d19c2663a69af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
